{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model if you're using the clustering function in a different context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Installing the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "#from sklearn import svc\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "Go to ServiceNow to export your training data in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'incident_230630_240111.csv' #replace the file path with the file that you will be using\n",
    "data = pd.read_csv(file_path, encoding='latin1').apply(lambda x: x.astype(str)).rename(columns={'problem_id.u_component': 'problem_component', 'inc_short_description': 'short_description'})\n",
    "description = data['short_description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(2,3))\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "cluster_model = KMeans(n_clusters=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    " \n",
    "model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING THE MODEL\n",
    "\n",
    "topic_model = BERTopic(min_topic_size=2, language=\"english\", calculate_probabilities=True,\n",
    "                       embedding_model=\"BAAI/bge-small-en-v1.5\",\n",
    "                       umap_model=umap_model,\n",
    "                       hdbscan_model=cluster_model,\n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                        top_n_words=5,\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.get('https://www.huggingface.co')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "topics = topic_model.fit_transform(description)\n",
    "topics_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Fine-Tuning with Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'WMS PRM Analysis.csv'\n",
    "data = pd.read_csv(file_path,encoding='cp1252').reset_index() \n",
    "training_data = data.dropna(subset=['H1']) \n",
    "training_data['H1'].value_counts().sort_index()\n",
    "training_data['groups'] = training_data.groupby('H1').ngroup()\n",
    "training_data['groups'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview of lablels\n",
    "\n",
    "category_names = training_data['H1'].unique()\n",
    "category_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetuning Parameters\n",
    "\n",
    "groups = training_data['groups'].to_numpy()\n",
    "docs = training_data['short_description'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuned Model\n",
    "\n",
    "topic_model = BERTopic(verbose=True).fit(docs, y=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Saving the Fine-Tuned Model as a Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save model in pickle file\n",
    "\n",
    "filename = open(\"wms_incidents_model.pkl\", 'wb') # rename the model based on the training context\n",
    "pickle.dump(topic_model, filename)\n",
    "filename.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Testing the Saved Model Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'incident_sla-051524-1850.csv' # replace with file for testing\n",
    "\n",
    "\n",
    "model_file = open('wms_incidents_model.pkl', 'rb')\n",
    "\n",
    "loaded_model = pickle.load(model_file)\n",
    "model_file.close()\n",
    "\n",
    "input_data = pd.read_csv(input_file_path, encoding='latin1').apply(lambda x: x.astype(str)).rename(columns={'problem_id.u_component': 'problem_component', 'inc_short_description': 'short_description'})\n",
    "\n",
    "docs_processing = input_data['short_description']\n",
    "topics, probs = loaded_model.fit_transform(docs_processing)\n",
    "\n",
    "result = loaded_model.get_topic_info()\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
